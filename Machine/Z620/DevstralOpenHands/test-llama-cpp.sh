#!/bin/bash

# llama.cpp Container Test Script
# Direct container testing with fixed parameters for easy modification

set -e

# Function to show usage
show_usage() {
    echo "ü¶ô llama.cpp Container Test Script"
    echo "=================================="
    echo ""
    echo "Usage: $0 [COMMAND]"
    echo ""
    echo "Commands:"
    echo "  up      Start llama.cpp container"
    echo "  down    Stop and remove container"
    echo "  logs    Show container logs"
    echo "  status  Show container status"
    echo "  test    Run API endpoint tests"
    echo "  help    Show this help message"
    echo ""
    echo "Examples:"
    echo "  $0 up       # Start container"
    echo "  $0 down     # Stop container"
    echo "  $0 logs     # View logs"
    echo "  $0 status   # Check status"
    echo "  $0 test     # Test API endpoints with performance metrics"
    echo ""
    echo "Configuration (Fixed Parameters):"
    echo "  ‚Ä¢ Container Name: llama-cpp-test"
    echo "  ‚Ä¢ Port: 11434"
    echo "  ‚Ä¢ Model: devstral-q4_k_m.gguf"
    echo "  ‚Ä¢ Model Path: ~/.models:/models"
    echo "  ‚Ä¢ GPU Layers: 41"
    echo "  ‚Ä¢ Context Size: 88832"
    echo "  ‚Ä¢ Threads: 12"
    echo "  ‚Ä¢ Batch Size: 1024"
    echo "  ‚Ä¢ Micro-batch Size: 512"
    echo "  ‚Ä¢ Flash Attention: Enabled"
    echo "  ‚Ä¢ Continuous Batching: Enabled"
    echo "  ‚Ä¢ Parallel Slots: 1"
    echo ""
    echo "API Endpoints:"
    echo "  ‚Ä¢ Health: http://localhost:11434/health"
    echo "  ‚Ä¢ Models: http://localhost:11434/v1/models"
    echo "  ‚Ä¢ Chat: http://localhost:11434/v1/chat/completions"
    echo "  ‚Ä¢ Completions: http://localhost:11434/v1/completions"
    echo ""
    echo "Performance Features:"
    echo "  ‚Ä¢ Tokens per second (TPS) calculation for all tests"
    echo "  ‚Ä¢ Token usage statistics (prompt + completion + total)"
    echo "  ‚Ä¢ Performance benchmark test with detailed metrics"
    echo "  ‚Ä¢ Response time measurement for all API calls"
    echo ""
    exit 0
}

# Function to start llama.cpp container
start_container() {
    echo "üöÄ Starting llama.cpp Container"
    echo "==============================="
    
    CONTAINER_NAME="llama-cpp-test"
    
    # Build the container if it doesn't exist
    if ! docker images | grep -q "llama-cpp-devstral"; then
        echo "üî® Building llama.cpp container..."
        docker build -t llama-cpp-devstral .
    fi
    
    # Check if container is already running
    if docker ps -q --filter "name=$CONTAINER_NAME" | grep -q .; then
        echo "‚ö†Ô∏è  Container is already running"
        show_status
        return 0
    fi
    
    # Remove existing container if it exists
    if docker ps -aq --filter "name=$CONTAINER_NAME" | grep -q .; then
        echo "üóëÔ∏è  Removing existing container..."
        docker rm $CONTAINER_NAME
    fi

    echo "üîÑ Starting llama.cpp container with fixed parameters..."
    echo "üìù Container Configuration:"
    echo "  ‚Ä¢ Model: /models/devstral-q4_k_m.gguf"
    echo "  ‚Ä¢ Host: 0.0.0.0"
    echo "  ‚Ä¢ Port: 11434"
    echo "  ‚Ä¢ GPU Layers: 41"
    echo "  ‚Ä¢ Context Size: 88832"
    echo "  ‚Ä¢ Threads: 12"
    echo "  ‚Ä¢ Batch Size: 1024"
    echo "  ‚Ä¢ Micro-batch Size: 512"
    echo "  ‚Ä¢ Flash Attention: Enabled"
    echo "  ‚Ä¢ Continuous Batching: Enabled"
    echo "  ‚Ä¢ Parallel Slots: 1"
    echo ""
    
    # Start container with fixed parameters (based on .env file)
    CONTAINER_ID=$(docker run -d \
        --name llama-cpp-test \
        --gpus all \
        -p 11434:11434 \
        -v ~/.models:/models \
        -e LLAMA_ARG_HOST=0.0.0.0 \
        -e LLAMA_ARG_PORT=11434 \
        -e LLAMA_ARG_MODEL=/models/devstral-q4_k_m.gguf \
        -e LLAMA_ARG_CTX_SIZE=88832 \
        -e LLAMA_ARG_N_GPU_LAYERS=41 \
        -e LLAMA_ARG_THREADS=12 \
        -e LLAMA_ARG_BATCH_SIZE=1024 \
        -e LLAMA_ARG_UBATCH_SIZE=512 \
        -e LLAMA_ARG_FLASH_ATTN=1 \
        -e LLAMA_ARG_CONT_BATCHING=1 \
        -e LLAMA_ARG_PARALLEL=1 \
        -e NVIDIA_VISIBLE_DEVICES=all \
        -e NVIDIA_DRIVER_CAPABILITIES=compute,utility \
        -e MODEL_ALIAS=devstral-2507 \
        llama-cpp-devstral)
    
    echo "‚úÖ Container started with ID: $CONTAINER_ID"
    echo "‚è≥ Waiting 5 seconds for container to initialize..."
    sleep 5
    
    # Check if container is still running after startup
    if ! docker ps -q --filter "name=$CONTAINER_NAME" | grep -q .; then
        echo "‚ùå Container stopped during startup"
        echo "üîç Container logs:"
        docker logs $CONTAINER_NAME
        echo ""
        echo "üîç Container status:"
        docker ps -a --filter "name=$CONTAINER_NAME"
        echo ""
        echo "‚ùå Container startup failed"
        exit 1
    fi
    
    # Wait for health endpoint to be ready
    echo "‚è≥ Waiting for llama.cpp server to be ready..."
    for i in {1..30}; do
        if curl -s http://localhost:11434/health >/dev/null 2>&1; then
            echo "‚úÖ llama.cpp server is ready!"
            break
        fi
        
        # Check if container is still running
        if ! docker ps -q --filter "name=$CONTAINER_NAME" | grep -q .; then
            echo "‚ùå Container stopped unexpectedly"
            echo "üîç Container logs:"
            docker logs --tail 20 $CONTAINER_NAME
            echo ""
            echo "üîç Container status:"
            docker ps -a --filter "name=$CONTAINER_NAME"
            exit 1
        fi
        
        if [ $i -eq 30 ]; then
            echo "‚ùå Server failed to start within 30 seconds"
            echo "üîç Container logs:"
            docker logs --tail 20 $CONTAINER_NAME
            echo ""
            echo "üîç Container status:"
            docker ps -a --filter "name=$CONTAINER_NAME"
            exit 1
        fi
        
        echo "‚è≥ Attempt $i/30 - waiting for server..."
        sleep 1
    done
    
    # Test basic endpoints
    echo "üîç Testing basic API endpoints..."
    
    # Test health endpoint
    HEALTH_RESPONSE=$(curl -s http://localhost:11434/health 2>/dev/null)
    if [ $? -eq 0 ] && [ -n "$HEALTH_RESPONSE" ]; then
        echo "‚úÖ Health endpoint: $HEALTH_RESPONSE"
    else
        echo "‚ùå Health endpoint failed"
        exit 1
    fi
    
    # Test models endpoint
    if curl -s http://localhost:11434/v1/models | jq . >/dev/null 2>&1; then
        echo "‚úÖ Models endpoint working"
    else
        echo "‚ùå Models endpoint failed"
        echo "üîç Raw response:"
        curl -s http://localhost:11434/v1/models
        echo ""
        exit 1
    fi
    
    echo ""
    echo "üéâ llama.cpp container is running successfully!"
    echo "  ‚Ä¢ Container: llama-cpp-test"
    echo "  ‚Ä¢ Health: http://localhost:11434/health"
    echo "  ‚Ä¢ Models: http://localhost:11434/v1/models"
    echo "  ‚Ä¢ API Base: http://localhost:11434/v1"
    echo ""
    echo "üß™ Run './test-llama-cpp.sh test' to test API endpoints"
}

# Function to stop container
stop_container() {
    echo "üõë Stopping llama.cpp Container"
    echo "==============================="
    
    CONTAINER_NAME="llama-cpp-test"
    
    # Stop container
    if docker ps -q --filter "name=$CONTAINER_NAME" | grep -q .; then
        echo "üîÑ Stopping container: $CONTAINER_NAME"
        docker stop $CONTAINER_NAME
    else
        echo "‚ÑπÔ∏è  Container is not running"
    fi

    # Remove container
    if docker ps -aq --filter "name=$CONTAINER_NAME" | grep -q .; then
        echo "üóëÔ∏è  Removing container: $CONTAINER_NAME"
        docker rm $CONTAINER_NAME
    else
        echo "‚ÑπÔ∏è  Container does not exist"
    fi
    
    echo "‚úÖ Container stopped and removed!"
}

# Function to show container logs
show_logs() {
    CONTAINER_NAME="llama-cpp-test"
    
    echo "üìã llama.cpp Container Logs"
    echo "=========================="
    
    if docker ps -q --filter "name=$CONTAINER_NAME" | grep -q .; then
        echo "üîç Container Logs (Running):"
        echo "----------------------------"
        docker logs --tail 50 $CONTAINER_NAME
    elif docker ps -aq --filter "name=$CONTAINER_NAME" | grep -q .; then
        echo "üîç Container Logs (Stopped):"
        echo "----------------------------"
        docker logs --tail 50 $CONTAINER_NAME
    else
        echo "‚ùå Container does not exist"
        echo "üí° Run './test-llama-cpp.sh up' to start the container"
    fi
}

# Function to show container status
show_status() {
    CONTAINER_NAME="llama-cpp-test"
    
    echo "üìä llama.cpp Container Status"
    echo "=============================="
    
    # Check container status
    if docker ps -q --filter "name=$CONTAINER_NAME" | grep -q .; then
        echo "‚úÖ Container is running"
        docker ps --filter "name=$CONTAINER_NAME" --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
        echo ""
        
        # Test endpoints
        echo "üîç API Endpoint Status:"
        echo "----------------------"
        
        # Health check
        if curl -s http://localhost:11434/health >/dev/null 2>&1; then
            HEALTH_STATUS=$(curl -s http://localhost:11434/health)
            echo "‚úÖ Health: $HEALTH_STATUS"
        else
            echo "‚ùå Health: Not responding"
        fi
        
        # Models endpoint
        if curl -s http://localhost:11434/v1/models >/dev/null 2>&1; then
            echo "‚úÖ Models: Available"
        else
            echo "‚ùå Models: Not responding"
        fi
        
    elif docker ps -aq --filter "name=$CONTAINER_NAME" | grep -q .; then
        echo "‚ùå Container exists but is stopped"
        docker ps -a --filter "name=$CONTAINER_NAME" --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
    else
        echo "‚ùå Container does not exist"
    fi
    
    echo ""
    echo "üåê API Endpoints:"
    echo "  ‚Ä¢ Health: http://localhost:11434/health"
    echo "  ‚Ä¢ Models: http://localhost:11434/v1/models"
    echo "  ‚Ä¢ Chat: http://localhost:11434/v1/chat/completions"
    echo "  ‚Ä¢ Completions: http://localhost:11434/v1/completions"
    echo ""
    echo "üìä Quick Test Commands:"
    echo "  curl http://localhost:11434/health"
    echo "  curl http://localhost:11434/v1/models | jq ."
    echo "  ./test-llama-cpp.sh test"
}

# Function to calculate tokens per second from response
calculate_tps() {
    local response="$1"
    
    # Extract token counts from response
    local completion_tokens=$(echo "$response" | jq -r '.usage.completion_tokens // 0' 2>/dev/null)
    local prompt_tokens=$(echo "$response" | jq -r '.usage.prompt_tokens // 0' 2>/dev/null)
    local total_tokens=$(echo "$response" | jq -r '.usage.total_tokens // 0' 2>/dev/null)
    
    # Check if llama.cpp includes timing information in the response
    local prompt_eval_duration=$(echo "$response" | jq -r '.usage.prompt_eval_duration // null' 2>/dev/null)
    local eval_duration=$(echo "$response" | jq -r '.usage.eval_duration // null' 2>/dev/null)
    local total_duration=$(echo "$response" | jq -r '.usage.total_duration // null' 2>/dev/null)
    
    # llama.cpp might also include these fields
    local timings_predicted_ms=$(echo "$response" | jq -r '.timings.predicted_ms // null' 2>/dev/null)
    local timings_predicted_n=$(echo "$response" | jq -r '.timings.predicted_n // null' 2>/dev/null)
    local timings_prompt_ms=$(echo "$response" | jq -r '.timings.prompt_ms // null' 2>/dev/null)
    local timings_prompt_n=$(echo "$response" | jq -r '.timings.prompt_n // null' 2>/dev/null)
    
    echo "üìä Token usage: ${prompt_tokens} prompt + ${completion_tokens} completion = ${total_tokens} total"
    
    # Try to calculate TPS using response timing data first
    if [ "$timings_predicted_ms" != "null" ] && [ "$timings_predicted_n" != "null" ] && [ "$timings_predicted_ms" != "0" ]; then
        # llama.cpp native timing format
        local tps=$(echo "scale=2; $timings_predicted_n * 1000 / $timings_predicted_ms" | bc -l 2>/dev/null)
        echo "‚ö° Performance: ${timings_predicted_n} tokens in ${timings_predicted_ms}ms = ${tps} tokens/sec"
        if [ "$timings_prompt_ms" != "null" ] && [ "$timings_prompt_n" != "null" ]; then
            local prompt_tps=$(echo "scale=2; $timings_prompt_n * 1000 / $timings_prompt_ms" | bc -l 2>/dev/null)
            echo "‚ö° Prompt eval: ${timings_prompt_n} tokens in ${timings_prompt_ms}ms = ${prompt_tps} tokens/sec"
        fi
    elif [ "$eval_duration" != "null" ] && [ "$completion_tokens" != "0" ] && [ "$eval_duration" != "0" ]; then
        # Duration in nanoseconds, convert to seconds
        local duration_sec=$(echo "scale=6; $eval_duration / 1000000000" | bc -l 2>/dev/null)
        local tps=$(echo "scale=2; $completion_tokens / $duration_sec" | bc -l 2>/dev/null)
        echo "‚ö° Performance: ${completion_tokens} tokens in ${duration_sec}s = ${tps} tokens/sec"
    else
        echo "‚ö° Performance: No timing data available in response"
        echo "üîç Available timing fields:"
        echo "$response" | jq -r '.usage, .timings' 2>/dev/null | head -10
    fi
}

# Function to run API tests
run_api_tests() {
    echo "üß™ Running llama.cpp API Tests"
    echo "=============================="
    
    # Check if container is running
    if ! docker ps -q --filter "name=llama-cpp-test" | grep -q .; then
        echo "‚ùå Container is not running"
        echo "üí° Run './test-llama-cpp.sh up' first"
        exit 1
    fi
    
    BASE_URL="http://localhost:11434"
    MODEL="devstral-2507"
    
    echo "üîç Testing OpenAI-compatible API endpoints with performance metrics..."
    echo ""
    
    # Test 1: Health check
    echo "üîç Test 1: Health Check"
    echo "========================"
    HEALTH_RESPONSE=$(curl -s "$BASE_URL/health" 2>/dev/null)
    if [ $? -eq 0 ] && [ -n "$HEALTH_RESPONSE" ]; then
        echo "‚úÖ Health check passed: $HEALTH_RESPONSE"
    else
        echo "‚ùå Health check failed"
        return 1
    fi
    echo ""
    
    # Test 2: Models endpoint
    echo "üîç Test 2: Models Endpoint"
    echo "=========================="
    if curl -s "$BASE_URL/v1/models" | jq . >/dev/null 2>&1; then
        echo "‚úÖ Models endpoint passed"
        echo "üìã Available models:"
        curl -s "$BASE_URL/v1/models" | jq -r '.data[].id'
    else
        echo "‚ùå Models endpoint failed"
        return 1
    fi
    echo ""
    
    # Test 3: Chat completion (non-streaming)
    echo "üîç Test 3: Chat Completion (Non-Streaming)"
    echo "==========================================="
    
    CHAT_RESPONSE=$(curl -s -X POST "$BASE_URL/v1/chat/completions" \
        -H "Content-Type: application/json" \
        -d "{
            \"model\": \"$MODEL\",
            \"messages\": [
                {\"role\": \"user\", \"content\": \"Hello! What is 2+2?\"}
            ],
            \"stream\": false,
            \"max_tokens\": 50
        }" 2>/dev/null)
    
    if echo "$CHAT_RESPONSE" | jq . >/dev/null 2>&1; then
        echo "‚úÖ Chat completion (non-streaming) passed"
        echo "üí¨ Response: $(echo "$CHAT_RESPONSE" | jq -r '.choices[0].message.content')"
        calculate_tps "$CHAT_RESPONSE"
    else
        echo "‚ùå Chat completion (non-streaming) failed"
        echo "üìã Response: $CHAT_RESPONSE"
    fi
    echo ""
    
    # Test 4: Generate completion (non-streaming)
    echo "üîç Test 4: Generate Completion (Non-Streaming)"
    echo "==============================================="
    
    GENERATE_RESPONSE=$(curl -s -X POST "$BASE_URL/v1/completions" \
        -H "Content-Type: application/json" \
        -d "{
            \"model\": \"$MODEL\",
            \"prompt\": \"The capital of France is\",
            \"max_tokens\": 10,
            \"stream\": false
        }" 2>/dev/null)
    
    if echo "$GENERATE_RESPONSE" | jq . >/dev/null 2>&1; then
        echo "‚úÖ Generate completion (non-streaming) passed"
        echo "üí¨ Response: $(echo "$GENERATE_RESPONSE" | jq -r '.choices[0].text')"
        calculate_tps "$GENERATE_RESPONSE"
    else
        echo "‚ùå Generate completion (non-streaming) failed"
        echo "üìã Response: $GENERATE_RESPONSE"
    fi
    echo ""
    
    # Test 5: Chat completion (streaming)
    echo "üîç Test 5: Chat Completion (Streaming)"
    echo "======================================"
    STREAM_OUTPUT=$(curl -s -X POST "$BASE_URL/v1/chat/completions" \
        -H "Content-Type: application/json" \
        -d "{
            \"model\": \"$MODEL\",
            \"messages\": [
                {\"role\": \"user\", \"content\": \"Count to 3\"}
            ],
            \"stream\": true,
            \"max_tokens\": 30
        }" 2>/dev/null)
    
    if echo "$STREAM_OUTPUT" | grep -q "data:"; then
        echo "‚úÖ Chat completion (streaming) passed"
        echo "üìã Sample streaming chunks:"
        echo "$STREAM_OUTPUT" | head -3
    else
        echo "‚ùå Chat completion (streaming) failed"
        echo "üìã Response: $STREAM_OUTPUT"
    fi
    echo ""
    
    # Test 6: Generate completion (streaming)
    echo "üîç Test 6: Generate Completion (Streaming)"
    echo "=========================================="
    STREAM_OUTPUT=$(curl -s -X POST "$BASE_URL/v1/completions" \
        -H "Content-Type: application/json" \
        -d "{
            \"model\": \"$MODEL\",
            \"prompt\": \"List 3 colors:\",
            \"max_tokens\": 20,
            \"stream\": true
        }" 2>/dev/null)
    
    if echo "$STREAM_OUTPUT" | grep -q "data:"; then
        echo "‚úÖ Generate completion (streaming) passed"
        echo "üìã Sample streaming chunks:"
        echo "$STREAM_OUTPUT" | head -3
    else
        echo "‚ùå Generate completion (streaming) failed"
        echo "üìã Response: $STREAM_OUTPUT"
    fi
    echo ""
    
    # Test 7: System prompt
    echo "üîç Test 7: System Prompt"
    echo "========================"
    
    SYSTEM_RESPONSE=$(curl -s -X POST "$BASE_URL/v1/chat/completions" \
        -H "Content-Type: application/json" \
        -d "{
            \"model\": \"$MODEL\",
            \"messages\": [
                {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in exactly 5 words.\"},
                {\"role\": \"user\", \"content\": \"What is your name?\"}
            ],
            \"stream\": false,
            \"max_tokens\": 20
        }" 2>/dev/null)
    
    if echo "$SYSTEM_RESPONSE" | jq . >/dev/null 2>&1; then
        echo "‚úÖ System prompt passed"
        echo "üí¨ Response: $(echo "$SYSTEM_RESPONSE" | jq -r '.choices[0].message.content')"
        calculate_tps "$SYSTEM_RESPONSE"
    else
        echo "‚ùå System prompt failed"
        echo "üìã Response: $SYSTEM_RESPONSE"
    fi
    echo ""
    
    # Test 8: Multi-turn conversation
    echo "üîç Test 8: Multi-turn Conversation"
    echo "=================================="
    
    MULTI_TURN_RESPONSE=$(curl -s -X POST "$BASE_URL/v1/chat/completions" \
        -H "Content-Type: application/json" \
        -d "{
            \"model\": \"$MODEL\",
            \"messages\": [
                {\"role\": \"user\", \"content\": \"My name is Alice\"},
                {\"role\": \"assistant\", \"content\": \"Hello Alice! Nice to meet you.\"},
                {\"role\": \"user\", \"content\": \"What is my name?\"}
            ],
            \"stream\": false,
            \"max_tokens\": 20
        }" 2>/dev/null)
    
    if echo "$MULTI_TURN_RESPONSE" | jq . >/dev/null 2>&1; then
        echo "‚úÖ Multi-turn conversation passed"
        echo "üí¨ Response: $(echo "$MULTI_TURN_RESPONSE" | jq -r '.choices[0].message.content')"
        calculate_tps "$MULTI_TURN_RESPONSE"
    else
        echo "‚ùå Multi-turn conversation failed"
        echo "üìã Response: $MULTI_TURN_RESPONSE"
    fi
    echo ""
    
    # Test 9: Performance Benchmark (Higher token count)
    echo "üîç Test 9: Performance Benchmark"
    echo "================================"
    
    BENCHMARK_RESPONSE=$(curl -s -X POST "$BASE_URL/v1/chat/completions" \
        -H "Content-Type: application/json" \
        -d "{
            \"model\": \"$MODEL\",
            \"messages\": [
                {\"role\": \"user\", \"content\": \"Write a detailed explanation of how machine learning works, including neural networks, training processes, and practical applications. Make it comprehensive but accessible.\"}
            ],
            \"stream\": false,
            \"max_tokens\": 200
        }" 2>/dev/null)
    
    if echo "$BENCHMARK_RESPONSE" | jq . >/dev/null 2>&1; then
        echo "‚úÖ Performance benchmark passed"
        echo "üí¨ Response preview: $(echo "$BENCHMARK_RESPONSE" | jq -r '.choices[0].message.content' | head -c 150)..."
        calculate_tps "$BENCHMARK_RESPONSE"
    else
        echo "‚ùå Performance benchmark failed"
        echo "üìã Response: $BENCHMARK_RESPONSE"
    fi
    echo ""
    
    echo "üéØ Test Summary"
    echo "==============="
    echo "‚úÖ Completed comprehensive API testing with performance metrics"
    echo "üîó OpenAI-compatible endpoints tested"
    echo "üìù Both streaming and non-streaming modes tested"
    echo "üé≠ System prompts and multi-turn conversations tested"
    echo "‚ö° Tokens per second (TPS) calculated for all non-streaming tests"
    echo "üöÄ Performance benchmark included for detailed TPS analysis"
    echo ""
    echo "üöÄ llama.cpp container is ready for use!"
}

# Main script logic
case "${1:-}" in
    "up"|"start")
        start_container
        ;;
    "down"|"stop")
        stop_container
        ;;
    "logs")
        show_logs
        ;;
    "status")
        show_status
        ;;
    "test")
        run_api_tests
        ;;
    "help"|"-h"|"--help")
        show_usage
        ;;
    "")
        show_usage
        ;;
    *)
        echo "‚ùå Unknown command: $1"
        echo ""
        show_usage
        ;;
esac
