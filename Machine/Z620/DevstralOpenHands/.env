# Simplified Docker Compose Environment Variables
# Based on test script learnings - only essential variables

# CUDA Configuration
CUDA_DOCKER_ARCH=61             # Pascal=61, Turing=75, Ampere=86, Ada=89, Hopper=90

# llama.cpp Server Configuration (from test-llama-cpp.sh)
LLAMA_ARG_PORT=11434            # Server port
LLAMA_ARG_MODEL=/models/devstral-q4_k_m.gguf    # Model path inside container
LLAMA_ARG_CTX_SIZE=88832        # Context window size
LLAMA_ARG_N_GPU_LAYERS=41       # GPU layers
LLAMA_ARG_THREADS=12            # CPU threads
LLAMA_ARG_BATCH_SIZE=1024       # Batch size
LLAMA_ARG_UBATCH_SIZE=512       # Micro-batch size
LLAMA_ARG_FLASH_ATTN=1          # Flash attention
LLAMA_ARG_CONT_BATCHING=1       # Continuous batching
LLAMA_ARG_PARALLEL=1            # Parallel slots

# OpenHands Configuration (from test-openhands.sh)
OPENHANDS_IMAGE=docker.all-hands.dev/all-hands-ai/openhands:0.49
OPENHANDS_VERSION=0.49
OPENHANDS_RUNTIME_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.49-nikolaik
OPENHANDS_RUNTIME_VERSION=0.49-nikolaik
OPENHANDS_PORT=3000
OPENHANDS_CONTAINER_NAME=openhands-test
OPENHANDS_LOG_ALL_EVENTS=true
OPENHANDS_WORKSPACE_BASE=/workspace
OPENHANDS_SANDBOX_TIMEOUT=120
OPENHANDS_RUNTIME=docker

# OpenWebUI Configuration (from test-openwebui.sh)
OPENWEBUI_PORT=8080
OPENWEBUI_WEBUI_AUTH=false
OPENWEBUI_ENABLE_OLLAMA_API=false
OPENWEBUI_ENABLE_OPENAI_API=true
OPENWEBUI_OPENAI_API_BASE_URL=http://host.docker.internal:11434/v1
OPENWEBUI_OPENAI_API_KEY=sk-no-key-required
OPENWEBUI_WEBUI_NAME="Devstral WebUI"
OPENWEBUI_DEFAULT_MODELS=devstral-small-2507
OPENWEBUI_DEFAULT_USER_ROLE=admin

# Model and GPU Configuration
MODEL_NAME=devstral-q4_k_m.gguf
MODEL_DIR=~/.models
MODEL_URL=https://huggingface.co/mistralai/Devstral-Small-2507_gguf/resolve/main/Devstral-Small-2507-Q4_K_M.gguf
NVIDIA_VISIBLE_DEVICES=all
