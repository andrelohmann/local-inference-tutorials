# Docker Compose Environment Variables
# Devstral + OpenHands Configuration

# CUDA Configuration
CUDA_DOCKER_ARCH=61             # Pascal=61, Turing=75, Ampere=86, Ada=89, Hopper=90
CUDA_VERSION=12.6.0
UBUNTU_VERSION=24.04

# llama.cpp Server Configuration
# Network Settings
LLAMA_ARG_HOST=0.0.0.0          # Server bind address (0.0.0.0 = all interfaces)
LLAMA_ARG_PORT=11434            # Server port (11434 = Ollama standard)

# Model Configuration
LLAMA_ARG_MODEL=/models/devstral-q4_k_m.gguf    # Path to model file inside container

# Context and Memory Settings
LLAMA_ARG_CTX_SIZE=88832        # Context window size (32k tokens - fits in GPU memory)
LLAMA_ARG_N_GPU_LAYERS=41       # Number of model layers to run on GPU (41 = all layers for Devstral, -1 = auto)

# Performance Settings
LLAMA_ARG_THREADS=12            # CPU threads for processing (match your total threads with hyperthreading)
#LLAMA_ARG_BATCH_SIZE=2048       # Batch size for prompt processing (reduced for memory)
#LLAMA_ARG_UBATCH_SIZE=1024      # Micro-batch size for generation (reduced for memory)
LLAMA_ARG_BATCH_SIZE=1024       # Batch size for prompt processing (reduced for memory)
LLAMA_ARG_UBATCH_SIZE=512      # Micro-batch size for generation (reduced for memory)

# Advanced Features
LLAMA_ARG_FLASH_ATTN=1          # Enable flash attention (1=on, 0=off) - faster processing
LLAMA_ARG_CONT_BATCHING=1       # Enable continuous batching (1=on, 0=off) - better throughput
LLAMA_ARG_PARALLEL=1            # Number of parallel processing slots (1 = single stream to save memory)

# OpenHands Configuration
OPENHANDS_VERSION=0.49
OPENHANDS_RUNTIME_VERSION=0.49-nikolaik
OPENHANDS_PORT=3000
OPENHANDS_LLM_MODEL=devstral
OPENHANDS_LLM_BASE_URL=http://llama-cpp-server:11434
OPENHANDS_LOG_ALL_EVENTS=true

# LLM Provider Configuration
LLM_API_VERSION=v1
LLM_CUSTOM_LLM_PROVIDER=openai
LLM_DROP_PARAMS=true

# OpenWebUI Configuration
OPENWEBUI_VERSION=latest
OPENWEBUI_PORT=8080
OPENWEBUI_OLLAMA_BASE_URL=http://llama-cpp-server:11434

# Sandbox Configuration (replaces deprecated WORKSPACE_* variables)
# SANDBOX_USER_ID will be set dynamically at runtime by start.sh
SANDBOX_USER_ID=1000
SANDBOX_VOLUMES=$PWD/workspace:/workspace:rw

# Model Configuration
MODEL_NAME=devstral-q4_k_m.gguf
MODEL_URL=https://huggingface.co/mistralai/Devstral-Small-2507_gguf/resolve/main/Devstral-Small-2507-Q4_K_M.gguf
MODEL_DIR=~/.models                  # User-specific model storage directory

# GPU Configuration
# Control which GPUs are visible to the container
# Options:
#   all         - Use all available GPUs (default)
#   0           - Use only GPU 0
#   1           - Use only GPU 1
#   0,1         - Use GPUs 0 and 1
#   none        - Disable GPU usage (CPU-only mode)
NVIDIA_VISIBLE_DEVICES=all

# NVIDIA driver capabilities (usually doesn't need to be changed)
NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Resource Limits
MEMORY_LIMIT=16g

# Development vs Production Configuration
# For development (CPU-only): use docker-compose.dev.yml
# For production (GPU): use docker-compose.yml (main file)
SHARED_MEMORY_SIZE=2g

# Health Check Configuration
HEALTHCHECK_START_PERIOD=60s     # Start health checks after 1 minute
HEALTHCHECK_INTERVAL=30s         # Health check frequency
HEALTHCHECK_TIMEOUT=10s          # Health check timeout
HEALTHCHECK_RETRIES=10           # Higher retries since we're always "healthy" during download
