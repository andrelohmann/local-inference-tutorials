# Docker Compose Environment Variables
# Devstral + OpenHands Configuration

# CUDA Configuration
CUDA_DOCKER_ARCH=61
CUDA_VERSION=12.6.0
UBUNTU_VERSION=24.04

# llama.cpp Server Configuration
LLAMA_HOST=0.0.0.0
LLAMA_PORT=11434
LLAMA_MODEL_PATH=/models/devstral-q4_k_m.gguf
LLAMA_CTX_SIZE=4096
LLAMA_N_GPU_LAYERS=35
LLAMA_THREADS=6
LLAMA_BATCH_SIZE=512
LLAMA_UBATCH_SIZE=512
LLAMA_PARALLEL=4

# OpenHands Configuration
OPENHANDS_VERSION=0.48
OPENHANDS_RUNTIME_VERSION=0.48-nikolaik
OPENHANDS_PORT=3000
OPENHANDS_LLM_MODEL=devstral
OPENHANDS_LLM_BASE_URL=http://llama-cpp-server:11434
OPENHANDS_WORKSPACE_BASE=/workspace
OPENHANDS_LOG_ALL_EVENTS=true

# Model Configuration
MODEL_NAME=devstral-q4_k_m.gguf
MODEL_URL=https://huggingface.co/mistralai/Devstral-Small-2507_gguf/resolve/main/Devstral-Small-2507-Q4_K_M.gguf

# GPU Configuration (for Pascal architecture)
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Resource Limits
MEMORY_LIMIT=16g
SHARED_MEMORY_SIZE=2g
