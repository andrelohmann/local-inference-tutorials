services:
  # Development version - CPU only llama.cpp server (no GPU requirements)
  llama-cpp-server:
    build:
      context: .
      dockerfile: Dockerfile
      target: server
      args:
        CUDA_DOCKER_ARCH: default  # CPU-only build for development
    container_name: llama-cpp-devstral-dev
    restart: unless-stopped
    environment:
      - LLAMA_ARG_HOST=${LLAMA_ARG_HOST}
      - LLAMA_ARG_PORT=${LLAMA_ARG_PORT}
      - LLAMA_ARG_MODEL=${LLAMA_ARG_MODEL}
      - LLAMA_ARG_CTX_SIZE=${LLAMA_ARG_CTX_SIZE}
      - LLAMA_ARG_N_GPU_LAYERS=0  # CPU-only for development
      - LLAMA_ARG_THREADS=${LLAMA_ARG_THREADS}
      - LLAMA_ARG_BATCH_SIZE=512   # Smaller batch size for CPU
      - LLAMA_ARG_UBATCH_SIZE=256  # Smaller micro-batch size for CPU
      - LLAMA_ARG_FLASH_ATTN=0    # Disable flash attention for CPU
      - LLAMA_ARG_CONT_BATCHING=1
      - LLAMA_ARG_PARALLEL=1      # Single processing slot for CPU
      - MODEL_URL=${MODEL_URL}
      - MODEL_FILE=${MODEL_NAME}
    ports:
      - "${LLAMA_ARG_PORT}:${LLAMA_ARG_PORT}"
    volumes:
      - ${MODEL_DIR}:/models
    networks:
      - devstral-network
    # No GPU runtime for development
    healthcheck:
      test: ["CMD", "/app/health-check.sh"]
      interval: ${HEALTHCHECK_INTERVAL:-30s}
      timeout: ${HEALTHCHECK_TIMEOUT:-10s}
      retries: ${HEALTHCHECK_RETRIES:-10}
      start_period: ${HEALTHCHECK_START_PERIOD:-60s}

  openhands:
    image: docker.all-hands.dev/all-hands-ai/openhands:${OPENHANDS_VERSION}
    container_name: openhands-dev
    restart: unless-stopped
    environment:
      - SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:${OPENHANDS_RUNTIME_VERSION}
      - SANDBOX_USER_ID=${SANDBOX_USER_ID}
      - SANDBOX_VOLUMES=${SANDBOX_VOLUMES}
      - LOG_ALL_EVENTS=${OPENHANDS_LOG_ALL_EVENTS}
      - RUNTIME=docker
      - LLM_MODEL=${OPENHANDS_LLM_MODEL}
      - LLM_BASE_URL=${OPENHANDS_LLM_BASE_URL}
      - LLM_API_KEY=dummy
      - LLM_EMBEDDING_MODEL=local
      - LLM_EMBEDDING_BASE_URL=${OPENHANDS_LLM_BASE_URL}
    ports:
      - "${OPENHANDS_PORT}:3000"
    volumes:
      - ./openhands-logs:/logs
      - /var/run/docker.sock:/var/run/docker.sock
      - ~/.openhands:/.openhands
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      llama-cpp-server:
        condition: service_healthy
    networks:
      - devstral-network

networks:
  devstral-network:
    driver: bridge
