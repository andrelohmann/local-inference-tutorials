#!/bin/bash

# Simplified Devstral + OpenHands Docker Compose Startup Script
# Based on test script learnings - streamlined for essential functionality

set -e

echo "ğŸš€ Starting Devstral + OpenHands Setup..."
echo "=================================================="

# Load environment variables
if [ -f .env ]; then
    source .env
    echo "âœ… Environment variables loaded"
else
    echo "âŒ Error: .env file not found"
    exit 1
fi

# Set dynamic SANDBOX_USER_ID (learned from test scripts)
export SANDBOX_USER_ID=$(id -u)
echo "ğŸ” Setting SANDBOX_USER_ID to: ${SANDBOX_USER_ID}"

echo ""
echo "ğŸ“‹ Configuration Summary:"
echo "  â€¢ CUDA Architecture: ${CUDA_DOCKER_ARCH}"
echo "  â€¢ Model: ${MODEL_NAME}"
echo "  â€¢ llama.cpp Port: ${LLAMA_ARG_PORT}"
echo "  â€¢ OpenHands Version: ${OPENHANDS_VERSION}"
echo "  â€¢ OpenHands Port: ${OPENHANDS_PORT}"
echo "  â€¢ OpenWebUI Port: ${OPENWEBUI_PORT}"
echo "  â€¢ User ID: ${SANDBOX_USER_ID}"
echo "  â€¢ Context Window: ${LLAMA_ARG_CTX_SIZE} tokens"
echo "  â€¢ GPU Layers: ${LLAMA_ARG_N_GPU_LAYERS}"
echo ""

# Check if Docker is running
if ! docker info > /dev/null 2>&1; then
    echo "âŒ Error: Docker is not running. Please start Docker first."
    exit 1
fi

# Check if NVIDIA Docker runtime is available
if ! docker run --rm --gpus all nvidia/cuda:12.6.0-base-ubuntu24.04 nvidia-smi > /dev/null 2>&1; then
    echo "âš ï¸  Warning: NVIDIA Docker runtime not available. GPU acceleration will not work."
    echo "   Please install NVIDIA Container Toolkit."
fi

# Create necessary directories (simplified from test scripts)
echo "ğŸ“ Creating required directories..."
mkdir -p ~/.models workspace openhands-logs openwebui-data ~/.openhands ~/.openhands/workspace

# Create OpenHands configuration (learned from test-openhands.sh)
echo "ğŸ”§ Creating OpenHands configuration files..."
cat > ~/.openhands/settings.json << EOF
{"language":"en","agent":"CodeActAgent","max_iterations":null,"security_analyzer":null,"confirmation_mode":false,"llm_model":"openai/devstral-small-2507","llm_api_key":"DEVSTRAL","llm_base_url":"http://llama-cpp-server:11434","remote_runtime_resource_factor":1,"secrets_store":{"provider_tokens":{}},"enable_default_condenser":true,"enable_sound_notifications":false,"enable_proactive_conversation_starters":false,"user_consents_to_analytics":false,"sandbox_base_container_image":null,"sandbox_runtime_container_image":null,"mcp_config":{"sse_servers":[],"stdio_servers":[],"shttp_servers":[]},"search_api_key":"","sandbox_api_key":null,"max_budget_per_task":null,"email":null,"email_verified":null}
EOF

cat > ~/.openhands/config.toml << EOF
[llm]
model = "openai/devstral-small-2507"
base_url = "http://llama-cpp-server:11434"
api_key = "dummy"
api_version = "v1"
custom_llm_provider = "openai"
drop_params = true
EOF

echo "âœ… OpenHands configuration files created"

# Model download (simplified from test scripts)
echo ""
echo "ğŸ“¥ Checking model availability..."
MODEL_PATH_EXPANDED="${MODEL_DIR/#\~/$HOME}/${MODEL_NAME}"

if [ ! -f "$MODEL_PATH_EXPANDED" ]; then
    echo "ğŸ“¥ Downloading model: ${MODEL_NAME}"
    echo "ğŸ”— URL: ${MODEL_URL}"
    echo "ğŸ“ Destination: ${MODEL_PATH_EXPANDED}"
    echo ""
    
    if ! wget --continue --progress=bar:force:noscroll \
            --user-agent="Mozilla/5.0 (compatible; wget)" \
            -O "${MODEL_PATH_EXPANDED}.tmp" \
            "${MODEL_URL}"; then
        echo "âŒ Download failed!"
        rm -f "${MODEL_PATH_EXPANDED}.tmp"
        exit 1
    fi
    
    mv "${MODEL_PATH_EXPANDED}.tmp" "$MODEL_PATH_EXPANDED"
    echo "âœ… Model download completed!"
else
    echo "âœ… Model already exists: $MODEL_PATH_EXPANDED"
fi

# Pull runtime container
echo ""
echo "ğŸ“¦ Pulling OpenHands runtime container..."
docker pull docker.all-hands.dev/all-hands-ai/runtime:${OPENHANDS_RUNTIME_VERSION}

# Start containers
echo ""
echo "ğŸ”§ Building and starting containers..."
SANDBOX_USER_ID=${SANDBOX_USER_ID} docker compose up --build -d

# Wait for services (simplified)
echo ""
echo "â³ Waiting for services to become ready..."

# Wait for llama.cpp server
echo "   â€¢ Waiting for llama.cpp server..."
for i in {1..60}; do
    if curl -s http://localhost:${LLAMA_ARG_PORT}/health >/dev/null 2>&1; then
        echo "   âœ… llama.cpp server is ready!"
        break
    fi
    if [ $i -eq 60 ]; then
        echo "   âŒ Timeout waiting for llama.cpp server"
        exit 1
    fi
    sleep 5
done

# Wait for OpenHands
echo "   â€¢ Waiting for OpenHands..."
for i in {1..30}; do
    if curl -s http://localhost:${OPENHANDS_PORT} >/dev/null 2>&1; then
        echo "   âœ… OpenHands is ready!"
        break
    fi
    if [ $i -eq 30 ]; then
        echo "   âŒ Timeout waiting for OpenHands"
        exit 1
    fi
    sleep 5
done

# Wait for OpenWebUI
echo "   â€¢ Waiting for OpenWebUI..."
for i in {1..20}; do
    if curl -s http://localhost:${OPENWEBUI_PORT} >/dev/null 2>&1; then
        echo "   âœ… OpenWebUI is ready!"
        break
    fi
    if [ $i -eq 20 ]; then
        echo "   âš ï¸  OpenWebUI timeout (continuing anyway)"
        break
    fi
    sleep 3
done

echo ""
echo "ğŸ‰ Setup Complete!"
echo "=================================================="
echo "ğŸŒ OpenHands Interface: http://localhost:${OPENHANDS_PORT}"
echo "ğŸŒ OpenWebUI Interface: http://localhost:${OPENWEBUI_PORT}" 
echo "ğŸ”— llama.cpp Server: http://localhost:${LLAMA_ARG_PORT}"
echo ""
echo "ğŸ“ Configuration:"
echo "   â€¢ User ID: $(id -u) (set in containers)"
echo "   â€¢ OpenHands config: ~/.openhands/config.toml"
echo "   â€¢ Workspace: ./workspace"
echo ""
echo "ğŸ›‘ To stop: docker compose down"
