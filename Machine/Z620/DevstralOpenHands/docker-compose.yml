services:
  llama-cpp-server:
    build:
      context: .
      dockerfile: Dockerfile
      target: server
      args:
        CUDA_DOCKER_ARCH: ${CUDA_DOCKER_ARCH}  # Use value from .env file
    container_name: llama-cpp-devstral
    restart: unless-stopped
    environment:
      - LLAMA_ARG_HOST=${LLAMA_ARG_HOST}
      - LLAMA_ARG_PORT=${LLAMA_ARG_PORT}
      - LLAMA_ARG_MODEL=${LLAMA_ARG_MODEL}
      - LLAMA_ARG_CTX_SIZE=${LLAMA_ARG_CTX_SIZE}
      - LLAMA_ARG_N_GPU_LAYERS=${LLAMA_ARG_N_GPU_LAYERS}
      - LLAMA_ARG_THREADS=${LLAMA_ARG_THREADS}
      - LLAMA_ARG_BATCH_SIZE=${LLAMA_ARG_BATCH_SIZE}
      - LLAMA_ARG_UBATCH_SIZE=${LLAMA_ARG_UBATCH_SIZE}
      - LLAMA_ARG_FLASH_ATTN=${LLAMA_ARG_FLASH_ATTN}
      - LLAMA_ARG_CONT_BATCHING=${LLAMA_ARG_CONT_BATCHING}
      - LLAMA_ARG_PARALLEL=${LLAMA_ARG_PARALLEL}
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES}
      - NVIDIA_DRIVER_CAPABILITIES=${NVIDIA_DRIVER_CAPABILITIES}
      - MODEL_URL=${MODEL_URL}
      - MODEL_FILE=${MODEL_NAME}
    ports:
      - "${LLAMA_ARG_PORT}:${LLAMA_ARG_PORT}"
    volumes:
      - ${MODEL_DIR}:/models
    networks:
      - devstral-network
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['${NVIDIA_VISIBLE_DEVICES:-all}']
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "/app/health-check.sh"]
      interval: ${HEALTHCHECK_INTERVAL:-30s}
      timeout: ${HEALTHCHECK_TIMEOUT:-10s}
      retries: ${HEALTHCHECK_RETRIES:-10}
      start_period: ${HEALTHCHECK_START_PERIOD:-60s}  # Start checking after 1 minute

  openhands:
    image: docker.all-hands.dev/all-hands-ai/openhands:${OPENHANDS_VERSION}
    container_name: openhands
    restart: unless-stopped
    environment:
      - SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:${OPENHANDS_RUNTIME_VERSION}
      - SANDBOX_USER_ID=${SANDBOX_USER_ID}
      - SANDBOX_VOLUMES=${SANDBOX_VOLUMES}
      - LOG_ALL_EVENTS=${OPENHANDS_LOG_ALL_EVENTS}
      - RUNTIME=docker
      - LLM_MODEL=${OPENHANDS_LLM_MODEL}
      - LLM_BASE_URL=${OPENHANDS_LLM_BASE_URL}
      - LLM_API_KEY=dummy
      - LLM_EMBEDDING_MODEL=local
      - LLM_EMBEDDING_BASE_URL=${OPENHANDS_LLM_BASE_URL}
    ports:
      - "${OPENHANDS_PORT}:3000"
    volumes:
      - ./openhands-logs:/logs
      - /var/run/docker.sock:/var/run/docker.sock
      - ~/.openhands:/.openhands
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      llama-cpp-server:
        condition: service_healthy
    networks:
      - devstral-network

networks:
  devstral-network:
    driver: bridge
