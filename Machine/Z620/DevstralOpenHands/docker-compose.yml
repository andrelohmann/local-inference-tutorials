services:
  llama-cpp-server:
    build:
      context: .
      dockerfile: Dockerfile
      target: server
      args:
        CUDA_DOCKER_ARCH: ${CUDA_DOCKER_ARCH}
    container_name: llama-cpp-devstral
    restart: unless-stopped
    environment:
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_PORT=${LLAMA_ARG_PORT}
      - LLAMA_ARG_MODEL=${LLAMA_ARG_MODEL}
      - LLAMA_ARG_CTX_SIZE=${LLAMA_ARG_CTX_SIZE}
      - LLAMA_ARG_N_GPU_LAYERS=${LLAMA_ARG_N_GPU_LAYERS}
      - LLAMA_ARG_THREADS=${LLAMA_ARG_THREADS}
      - LLAMA_ARG_BATCH_SIZE=${LLAMA_ARG_BATCH_SIZE}
      - LLAMA_ARG_UBATCH_SIZE=${LLAMA_ARG_UBATCH_SIZE}
      - LLAMA_ARG_FLASH_ATTN=${LLAMA_ARG_FLASH_ATTN}
      - LLAMA_ARG_CONT_BATCHING=${LLAMA_ARG_CONT_BATCHING}
      - LLAMA_ARG_PARALLEL=${LLAMA_ARG_PARALLEL}
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES}
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - MODEL_URL=${MODEL_URL}
      - MODEL_FILE=${MODEL_NAME}
      - MODEL_ALIAS=devstral-small-2507
    ports:
      - "${LLAMA_ARG_PORT}:${LLAMA_ARG_PORT}"
    volumes:
      - ${MODEL_DIR}:/models
    networks:
      - devstral-network
    runtime: nvidia
    healthcheck:
      test: ["CMD", "/app/health-check.sh"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s

  openhands:
    image: ${OPENHANDS_IMAGE}
    container_name: ${OPENHANDS_CONTAINER_NAME}
    restart: unless-stopped
    environment:
      - SANDBOX_RUNTIME_CONTAINER_IMAGE=${OPENHANDS_RUNTIME_IMAGE}
      - LOG_ALL_EVENTS=${OPENHANDS_LOG_ALL_EVENTS}
      - WORKSPACE_BASE=${OPENHANDS_WORKSPACE_BASE}
      - SANDBOX_USER_ID=${SANDBOX_USER_ID}
      - SANDBOX_TIMEOUT=${OPENHANDS_SANDBOX_TIMEOUT}
      - RUNTIME=${OPENHANDS_RUNTIME}
    ports:
      - "${OPENHANDS_PORT}:3000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ~/.openhands:/.openhands
      - ~/.openhands/workspace:/workspace
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      llama-cpp-server:
        condition: service_healthy
    networks:
      - devstral-network

  openwebui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: openwebui
    restart: unless-stopped
    environment:
      - PUID=${SANDBOX_USER_ID}
      - PGID=${SANDBOX_USER_ID}
      - WEBUI_AUTH=${OPENWEBUI_WEBUI_AUTH}
      - ENABLE_OLLAMA_API=${OPENWEBUI_ENABLE_OLLAMA_API}
      - ENABLE_OPENAI_API=${OPENWEBUI_ENABLE_OPENAI_API}
      - OPENAI_API_BASE_URL=${OPENWEBUI_OPENAI_API_BASE_URL}
      - OPENAI_API_KEY=${OPENWEBUI_OPENAI_API_KEY}
      - WEBUI_NAME=${OPENWEBUI_WEBUI_NAME}
      - DEFAULT_MODELS=${OPENWEBUI_DEFAULT_MODELS}
      - DEFAULT_USER_ROLE=${OPENWEBUI_DEFAULT_USER_ROLE}
    ports:
      - "${OPENWEBUI_PORT}:8080"
    volumes:
      - ~/.openwebui:/app/backend/data
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      llama-cpp-server:
        condition: service_healthy
    networks:
      - devstral-network

networks:
  devstral-network:
    driver: bridge
